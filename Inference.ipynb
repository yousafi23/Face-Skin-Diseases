{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22062,"status":"ok","timestamp":1690795985909,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"xWTWEuuKbQfu","outputId":"69ec2d5a-67cd-410a-db7d-d87336679ae8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":9749,"status":"ok","timestamp":1690795995651,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"9egmNtLu7Y7w"},"outputs":[],"source":["import base64\n","import io\n","import torch\n","from io import StringIO\n","from io import BytesIO\n","from PIL import Image\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from PIL import Image\n","import ast\n","import os\n","import cv2\n","import albumentations as A\n","import numpy as np"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1690796012283,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"h0iubZ0FAmKu","outputId":"7ff53321-225a-4b97-d8fe-b0ceea4b652a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab_Skin_Disease\n"]}],"source":["%cd /content/drive/MyDrive/Colab_Skin_Disease/"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1690796012284,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"MidtQj6i452K"},"outputs":[],"source":["def base64_to_image(base64_string):\n","    image_data = base64.b64decode(base64_string)\n","    image_buffer = BytesIO(image_data)\n","    image = Image.open(image_buffer)\n","    return image"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":45,"status":"ok","timestamp":1690796012285,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"LUqchSD74mEy"},"outputs":[],"source":["def inference(model, img, conf, iou,img_size):\n","  model.conf = conf\n","  model.iou = iou\n","  # img = base64_to_image(img)\n","  results = model(img,size=img_size)\n","  results.print()\n","  results.show(labels=False)\n","  df = results.pandas().xyxy[0]\n","  return df"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1690796012286,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"DgRk_-hB5fI3"},"outputs":[],"source":["def multi_inference(model, imgs, conf, iou,img_size):\n","  model.conf = conf\n","  model.iou = iou\n","  results = model(imgs,size=img_size)\n","  results.print()\n","  results.show(labels=False)\n","  # results.save()\n","  df = results.pandas().xyxy[0]\n","  # return df"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1690796012287,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"dceAb9jJStgB"},"outputs":[],"source":["def df_to_dict(df):\n","  # Convert DataFrame to a list of dictionaries\n","  data_dict = df.to_dict(orient='records')\n","  return data_dict"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1690796012288,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"90hdE3FNOrgX"},"outputs":[],"source":["def plot_annots(df,image_path):\n","  img = Image.open(image_path)\n","  fig, ax = plt.subplots(figsize=(5, 5))\n","  ax.imshow(img)\n","  ax.axis('off')\n","\n","  for index,row in df.iterrows():\n","    xmin = float(row['xmin'])\n","    xmax = float(row['xmax'])\n","    ymin = float(row['ymin'])\n","    ymax = float(row['ymax'])\n","\n","    print(xmax,xmin,ymax,ymin)\n","    rect = patches.Rectangle((xmin, ymin), (xmax-xmin),(ymax-ymin), linewidth=0.5, edgecolor='r')\n","    ax.add_artist(rect)\n","\n","  plt.show()"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1690796012289,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"O2uq95oLaMG7"},"outputs":[],"source":["def plot(image_path, annots, key):\n","    img = Image.open(image_path)\n","    fig, ax = plt.subplots(1, 2, figsize=(8, 5))\n","    ax[0].imshow(img)\n","    ax[0].axis('off')\n","    for i in annots:\n","        xmin, ymin, xmax, ymax = i[\"xmin\"],i[\"ymin\"], i[\"xmax\"], i[\"ymax\"]\n","\n","        rect = patches.Rectangle((xmin, ymin), (xmax-xmin), (ymax-ymin), linewidth=0.5, edgecolor='r')\n","        ax[0].add_artist(rect)\n","    print(xmin, ymin, xmax, ymax)\n","    print(key)\n","    key = Image.open(key)\n","    ax[1].imshow(key)\n","    ax[1].axis('off')\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":44,"status":"ok","timestamp":1690796012289,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"O895QyhFoUZI"},"outputs":[],"source":["def face_crop(face_cascade,img_path):\n","    image = cv2.imread(img_path)\n","\n","    # Convert the image to grayscale\n","    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","    # Perform face detection\n","    faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(15, 15))\n","\n","    if len(faces) > 0:\n","        # Select the first detected face\n","        (x, y, w, h) = faces[0]\n","\n","        # Draw rectangle around the detected face\n","        # cv2.rectangle(image, (x-50, y-50), (x+w+50, y+h+50), (0, 0, 255), 2)\n","\n","        # Crop the image within the bounding box\n","        cropped_image = image[max(0, y-50):y+h+50, max(0, x-50):x+w+50]\n","\n","        # Convert the cropped image from BGR to RGB for displaying in matplotlib\n","        cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n","\n","        # Display the cropped image\n","        # plt.imshow(cropped_image_rgb)\n","        # plt.axis('off')\n","        # plt.show()\n","        return cropped_image_rgb\n","    else:\n","        print(\"No face detected in the image.\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o6p0cqKtOFv6"},"outputs":[],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1403,"status":"ok","timestamp":1690800969241,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"tkkasKYcqbid","outputId":"c18c2241-4880-4125-8072-5cb2d6769bde"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n","YOLOv5 ðŸš€ 2023-7-31 Python-3.10.6 torch-2.0.1+cu118 CPU\n","\n","Fusing layers... \n","Model summary: 212 layers, 20865057 parameters, 0 gradients\n","Adding AutoShape... \n"]}],"source":["weights = '/content/drive/MyDrive/Colab_Skin_Disease/yolov5/runs/train/exp94/weights/best.pt'\n","model = torch.hub.load('ultralytics/yolov5', 'custom', path = weights)"]},{"cell_type":"code","execution_count":24,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1690800969244,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"WPHwk1NX617V"},"outputs":[],"source":["file_paths = []\n","for dirpath,_,filenames in os.walk('/content/drive/MyDrive/Colab_Skin_Disease/TEST Images/'):\n","  for f in filenames:\n","      file_paths.append(os.path.abspath(os.path.join(dirpath, f)))"]},{"cell_type":"code","execution_count":25,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1690800971355,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"},"user_tz":-300},"id":"ooATf3qg4mAu"},"outputs":[],"source":["#converting img to base64 just for testing\n","# img_path = '/content/drive/MyDrive/Colab_Skin_Disease/via-2.0.12/kkkk2.jpg'\n","img_path = '/content/drive/MyDrive/Colab_Skin_Disease/TEST Images/IMG_1566.jpg'\n","\n","# with open(img_path, \"rb\") as img_file:\n","#     img_64 = base64.b64encode(img_file.read())"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/","output_embedded_package_id":"1SHuvfCnUmC64QSrCYWA3epqoC403DDLa","height":919},"id":"imc7Xjbm4l_W","outputId":"3c461b22-5005-42a8-e586-c1c8592d9caa","executionInfo":{"status":"ok","timestamp":1690801039330,"user_tz":-300,"elapsed":37748,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# image = Image.open(img_path)\n","image =  plt.imread(img_path)\n","\n","## Load the pre-trained face detection cascade\n","# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","# image = face_crop(face_cascade,img_path)\n","\n","transform = A.Compose([\n"," A.CLAHE(clip_limit=(2,2), tile_grid_size=(8,8),always_apply=True),\n","#  A.RandomRotate90(always_apply=True)\n","])\n","# image = np.array(image)\n","image = transform(image=image)['image']\n","\n","df = inference(model,image,conf=0.1,iou=0.2,img_size=640)\n","display(df)\n","plot_annots(df=df,image_path=img_path)\n","\n","# annots = df_to_dict(df)\n","# key = '/content/drive/MyDrive/Colab_Skin_Disease/All_diseases_key.drawio.png'\n","# print(annots)\n","# plot(img_path,annots,key)"]},{"cell_type":"code","execution_count":28,"metadata":{"colab":{"output_embedded_package_id":"1uocnoLtWW1WmlByWGMNFAn1uTofulzWN","base_uri":"https://localhost:8080/","height":1000},"id":"hopbDYmxj-Uf","outputId":"7f24d95f-af66-49b4-f8b2-57ff322c2850","executionInfo":{"status":"ok","timestamp":1690801144645,"user_tz":-300,"elapsed":78848,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["for path in file_paths:\n","  # image =  plt.imread(path)\n","\n","  # transform = A.Compose([\n","  #   A.CLAHE(clip_limit=(2,2), tile_grid_size=(8,8),always_apply=True),\n","  #   # A.RandomRotate90(always_apply=True)\n","  # ])\n","  # trans_image = transform(image=image)['image']\n","\n","  # df = inference(model,trans_image,conf=0.15,iou=0.2,img_size=1024)\n","  # # break\n","  multi_inference(model,path,conf=0.1,iou=0.25,img_size=640)"]},{"cell_type":"code","execution_count":29,"metadata":{"id":"16RHqpMf8phc","colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"12VrLe6kfTvPK7S75jD_vbrjOstxkPvmc"},"executionInfo":{"status":"ok","timestamp":1690801611563,"user_tz":-300,"elapsed":73772,"user":{"displayName":"Yousaf Irshad","userId":"11754361587066077720"}},"outputId":"cc46b2ca-7795-4c4f-cc8a-36dd4dc935ee"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","for path in file_paths:\n","   image = face_crop(face_cascade,path)\n","   if image is not None:\n","      multi_inference(model,image,conf=0.15,iou=0.2,img_size=10)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xo-UFoeRgnsb"},"outputs":[],"source":["import cv2\n","import matplotlib.pyplot as plt\n","\n","# Load the pre-trained face detection cascade\n","face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n","\n","# Load the image\n","img_path = '/content/drive/MyDrive/Colab_Skin_Disease/TEST Images/IMG_1566.jpg'\n","image = cv2.imread(img_path)\n","\n","# Convert the image to grayscale\n","gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n","\n","# Perform face detection\n","faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=3, minSize=(30, 30))\n","\n","if len(faces) > 0:\n","    # Select the first detected face\n","    (x, y, w, h) = faces[0]\n","\n","    # Draw rectangle around the detected face\n","    cv2.rectangle(image, (x-50, y-50), (x+w+50, y+h+50), (0, 0, 255), 2)\n","\n","    # Crop the image within the bounding box\n","    # cropped_image = image[max(0, y-50):y+h+50, max(0, x-50):x+w+50]\n","\n","    # Convert the cropped image from BGR to RGB for displaying in matplotlib\n","    # cropped_image_rgb = cv2.cvtColor(cropped_image, cv2.COLOR_BGR2RGB)\n","\n","    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","    # Display the cropped image\n","    plt.imshow(image)\n","    plt.axis('off')\n","    plt.show()\n","else:\n","    print(\"No face detected in the image.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tP08XKAxRPUN"},"outputs":[],"source":["from pixellib.tune_bg import alter_bg\n","\n","change_bg = alter_bg(model_type = \"pb\")\n","change_bg.load_pascalvoc_model(\"../input/dataset/xception_pascalvoc.pb\")\n","output = change_bg.change_bg_img(f_image_path = \"../input/images/c5afc3a4_2285_11ec_b946_80e650049a6e.png\",b_image_path = \"../input/images/bg.jpeg\", detect = \"car\")\n","cv2.imwrite(\"img.jpg\", output)\n","im_rgb = cv2.cvtColor(output, cv2.COLOR_RGB2BGR)\n","image = Image.fromarray(im_rgb)\n","image"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"40yoSq26TeLP"},"outputs":[],"source":["import pixellib\n","from pixellib.semantic import semantic_segmentation\n","\n","segment_image = semantic_segmentation()\n","segment_image.load_pascalvoc_model(\"deeplabv3_xception_tf_dim_ordering_tf_kernels.h5\")\n","segment_image.segmentAsPascalvoc(\"path_to_image\", output_image_name = \"path_to_output_image\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IK0W_PVzU5le"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}